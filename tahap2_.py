# -*- coding: utf-8 -*-
"""Tahap2..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Ga_stMwZ1cNSceBbHKV7OfINoyJuw_x
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('winequality-red.csv')
print("\nFitur dan tipe data dalam dataset:\n", df.dtypes)

# Pisahkan fitur dan label
X_features = df.drop(columns=["quality"], errors='ignore')

# Standarisasi data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_features)

# Konversi kembali ke DataFrame agar seaborn tidak error
X_scaled_df = pd.DataFrame(X_scaled, columns=X_features.columns)

# Menentukan jumlah optimal klaster menggunakan Elbow Method
inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot Elbow Method
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o', linestyle='--')
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

# Melatih model K-Means dengan jumlah cluster optimal
optimal_k = 6  # Sesuaikan dengan hasil Elbow Method
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

# Membuat DataFrame dari X_scaled dan menambahkan kolom 'Cluster'
X_scaled_df = pd.DataFrame(X_scaled)
X_scaled_df['Cluster'] = clusters

# Memeriksa distribusi anggota cluster
print(pd.Series(clusters).value_counts())

# Menggunakan value_counts() untuk mendapatkan distribusi anggota cluster
cluster_counts = pd.Series(clusters).value_counts().sort_index()

# Menggambar histogram / bar plot dengan menggunakan hue sesuai saran dari FutureWarning
plt.figure(figsize=(8, 6))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis', hue=cluster_counts.index, legend=True)

# Menambahkan judul dan label
plt.title('Distribusi Anggota Cluster')
plt.xlabel('Cluster')
plt.ylabel('Jumlah Anggota')
plt.show()

# Memeriksa distribusi anggota cluster
cluster_counts = pd.Series(clusters).value_counts().sort_index()
print(cluster_counts)

# Plot distribusi anggota cluster
plt.figure(figsize=(8, 6))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')
plt.title('Distribusi Anggota Cluster')
plt.xlabel('Cluster')
plt.ylabel('Jumlah Anggota')
plt.show()

# Visualisasi hasil klasterisasi dengan scatterplot untuk semua kombinasi fitur
print("\nMenampilkan scatter plot untuk semua kombinasi dua fitur...")
sns.pairplot(X_scaled_df, hue="Cluster", palette="viridis", diag_kind="hist", markers=["o", "s", "D"])
plt.suptitle("Visualisasi Klasterisasi K-Means pada Semua Kombinasi Fitur", y=1.02)
plt.show()

from sklearn.decomposition import PCA
import numpy as np

# Lakukan PCA tanpa menentukan jumlah komponen terlebih dahulu
pca_full = PCA()
pca_full.fit(X_scaled)

# Dapatkan explained variance ratio
explained_variance_ratio = pca_full.explained_variance_ratio_

# Pilih jumlah komponen PCA berdasarkan threshold 95% variance
optimal_components = np.argmax(np.cumsum(explained_variance_ratio) >= 0.95) + 1
print(f"Optimal jumlah komponen PCA: {optimal_components}")

# Reduksi dimensi menggunakan jumlah komponen optimal
pca = PCA(n_components=optimal_components)
X_pca = pca.fit_transform(X_scaled)

# Menampilkan explained variance ratio dari PCA terpilih
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance Ratio: {explained_variance}")
print(f"Total Explained Variance: {np.sum(explained_variance):.4f}")

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Pastikan X_scaled sudah didefinisikan sebelumnya

results = []  # List untuk menyimpan hasil evaluasi

k = 6  # Tetapkan jumlah klaster

for n_components in range(2, optimal_components + 1):
    # Reduksi dimensi menggunakan PCA dengan jumlah komponen tertentu
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)

    # Klasterisasi K-Means
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels = kmeans.fit_predict(X_pca)

    # Evaluasi klasterisasi
    silhouette = silhouette_score(X_pca, labels)
    davies_bouldin = davies_bouldin_score(X_pca, labels)
    calinski_harabasz = calinski_harabasz_score(X_pca, labels)

    # Simpan hasil evaluasi dalam list
    results.append({
        "PCA Components": n_components,
        "Silhouette Score": silhouette,
        "Davies-Bouldin Index": davies_bouldin,
        "Calinski-Harabasz Index": calinski_harabasz
    })

# Konversi hasil evaluasi ke dalam DataFrame
results_df = pd.DataFrame(results)

print("\nPerbandingan Evaluasi Klasterisasi dengan Berbagai Komponen PCA:")
print(results_df)

# Klasterisasi K-Means dengan input data PCA =2
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_pca)

# Visualisasi hasil klasterisasi
plt.figure(figsize=(8,6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6, edgecolors='k')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label="Centroid")
plt.legend()
plt.colorbar(scatter, label="Cluster")
plt.title(f"Visualisasi Klasterisasi K-Means dengan PCA (K={k})")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# Salin data asli sebelum standarisasi
df_original = df.copy()

# Tambahkan label klaster ke data asli
df_original["Cluster"] = labels

# Hitung statistik deskriptif untuk setiap klaster
summary = df_original.groupby("Cluster").agg(["mean", "std", "min", "max", "median"])

# Membulatkan angka agar lebih rapi
summary = summary.round(2)

# Menampilkan tabel per fitur dengan indeks fitur dan pemisah
print("\nStatistik Deskriptif untuk Setiap Klaster (Per Fitur):")
for idx, feature in enumerate(df_original.columns[:-1]):  # Menambahkan indeks fitur
    print("\n" + "-" * 50)  # Pemisah antar fitur
    print(f"Feature {idx+1}: {feature}")  # Menampilkan indeks fitur
    print(summary[feature].to_string(index=True))  # Menampilkan tabel hanya untuk fitur tertentu

    # Membuat box plot untuk melihat distribusi per klaster
    plt.figure(figsize=(7, 5))
    sns.boxplot(x="Cluster", y=feature, data=df_original, hue="Cluster", palette="Set2", legend=False)
    plt.title(f"Distribusi {feature} per Klaster")
    plt.xlabel("Cluster")
    plt.ylabel(feature)
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

# Hitung rata-rata setiap fitur untuk setiap klaster
mean_values = df_original.groupby("Cluster").mean()

# Membuat line chart
plt.figure(figsize=(10, 6))

# Loop untuk membuat satu garis per klaster
for cluster in mean_values.index:
    plt.plot(mean_values.columns, mean_values.loc[cluster], marker="o", linestyle="-", label=f"Cluster {cluster}")

# Menyesuaikan tampilan
plt.title("Rata-rata Setiap Fitur per Klaster")
plt.xlabel("Fitur")
plt.ylabel("Rata-rata Nilai")
plt.xticks(rotation=45)  # Rotasi label fitur agar lebih rapi
plt.legend(title="Cluster")
plt.grid(True, linestyle="--", alpha=0.6)

# Menampilkan plot
plt.show()

